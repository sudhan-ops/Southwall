import { createClient } from "@supabase/supabase-js";
import { supabase, supabaseAnonKey } from "./supabase";
import type {
  AppModule,
  Asset,
  AttendanceEvent,
  AttendanceSettings,
  BackOfficeIdSeries,
  CompOffLog,
  EnrollmentRules,
  Entity,
  ExtraWorkLog,
  Holiday,
  HolidayListItem,
  Insurance,
  InvoiceData,
  IssuedTool,
  LeaveBalance,
  LeaveRequest,
  Location,
  LocationLog,
  ManpowerDetail,
  MasterGentsUniforms,
  MasterLadiesUniforms,
  MasterTool,
  MasterToolsList,
  Notification,
  OnboardingData,
  Organization,
  OrganizationGroup,
  PatrolDailyScore,
  PatrolLog,
  PatrolQRCode,
  PerfiosVerificationData,
  Policy,
  RecurringHolidayRule,
  Role,
  SalaryChangeRequest,
  SiteConfiguration,
  SiteGentsUniformConfig,
  SiteLadiesUniformConfig,
  SiteStaff,
  SiteStaffDesignation,
  SiteUniformDetailsConfig,
  SubmissionCostBreakdown,
  SupportTicket,
  Task,
  TicketComment,
  TicketPost,
  UniformRequest,
  UniformRequestItem,
  UploadedFile,
  User,
  UserRole,
  VerificationResult,
} from "../types";
// FIX: Add 'startOfMonth' and 'endOfMonth' to date-fns import to resolve errors.
import {
  differenceInCalendarDays,
  endOfMonth,
  format,
  startOfMonth,
} from "date-fns";
import { GoogleGenAI, Modality, Type } from "@google/genai";

const ONBOARDING_DOCS_BUCKET = "onboarding-documents";
const AVATAR_BUCKET = "avatars";
const SUPPORT_ATTACHMENTS_BUCKET = "support-attachments";
// Buckets for storing branding assets.  These buckets should be created
// in your Supabase project via the Storage interface.  The `logo`
// bucket stores the active and default logos displayed throughout the
// application.  The `background` bucket stores the carousel images for
// the login screen.  Policies should permit public read and
// authenticated upload/update/delete.
const LOGO_BUCKET = "logo";
const BACKGROUND_BUCKET = "background";

// Bucket used to store attachments uploaded when marking tasks complete.  If this
// bucket does not already exist in your Supabase project, create it via the
// Supabase web console or CLI.  All task completion photos will be stored
// here.  Alternatively you can reuse SUPPORT_ATTACHMENTS_BUCKET if you prefer.
const TASK_ATTACHMENTS_BUCKET = "task-attachments";

// Resolve the Google GenAI API key from Vite environment variables.  When running
// locally without a real key, the application should still boot and render
// normally; AI-powered features will be disabled.  If a key is provided, the
// GoogleGenAI client is instantiated for use by downstream functions.  If the
// key is missing, log a warning and set `ai` to null.  Downstream functions
// should check for a null `ai` and provide sensible fallbacks rather than
// throwing.
const apiKey = import.meta.env.VITE_API_KEY;
let ai: GoogleGenAI | null = null;
if (!apiKey) {
  console.warn(
    "VITE_API_KEY for Google GenAI is not set in the environment variables. " +
      "AI-powered features (document extraction, name cross‑verification, fingerprint detection, document enhancement) will be disabled.",
  );
} else {
  ai = new GoogleGenAI({ apiKey });
}

// --- Helper Functions ---

const processUrlsForDisplay = (obj: any): any => {
  if (obj === null || typeof obj !== "object") return obj;
  if (Array.isArray(obj)) return obj.map(processUrlsForDisplay);

  const newObj = { ...obj };
  // Check if the object looks like our UploadedFile structure with a path
  if (typeof newObj.name === "string" && typeof newObj.path === "string") {
    const bucket = newObj.path.startsWith("avatars/")
      ? AVATAR_BUCKET
      : ONBOARDING_DOCS_BUCKET;
    const { data: { publicUrl } } = supabase.storage.from(bucket).getPublicUrl(
      newObj.path,
    );
    newObj.preview = publicUrl;
    newObj.url = publicUrl;
  }

  // Recursively process nested objects
  for (const key in newObj) {
    newObj[key] = processUrlsForDisplay(newObj[key]);
  }
  return newObj;
};

/**
 * Recursively process a mixed object structure and upload any File objects to
 * Supabase Storage.  For each file encountered, the file is uploaded to the
 * onboarding documents bucket and a corresponding record is inserted into
 * the user_documents table.  The returned object mirrors the input but
 * replaces File objects with plain metadata objects containing name,
 * type, size and the storage path.  If the input contains nested arrays
 * or objects, those structures are preserved.
 *
 * @param obj        Arbitrary object containing primitive values or nested
 *                   objects/arrays.  Any property named `file` with a
 *                   File instance will be uploaded.
 * @param userId     The ID of the user uploading the file.  Used to
 *                   construct storage paths and associate the document with
 *                   the user_documents table.
 * @param submissionId The onboarding submission ID; helps group files by
 *                   submission when provided.  May be an empty string.
 */
const processFilesForUpload = async (
  obj: any,
  userId: string,
  submissionId: string,
): Promise<any> => {
  if (obj === null || typeof obj !== "object") return obj;
  // Handle arrays by processing each element
  if (Array.isArray(obj)) {
    return Promise.all(
      obj.map((item) => processFilesForUpload(item, userId, submissionId)),
    );
  }

  const newObj: any = { ...obj };
  // If the object has a File to upload
  if (newObj.file instanceof File) {
    const file: File = newObj.file;
    // Construct a unique storage path using the userId and submissionId
    const filePath = `${userId}/documents/${Date.now()}_${file.name}`;
    // Upload the file to the onboarding documents bucket
    const { error: uploadError } = await supabase.storage.from(
      ONBOARDING_DOCS_BUCKET,
    ).upload(filePath, file);
    if (uploadError) throw uploadError;
    // Insert a record into the user_documents table.  Use a best‑effort
    // approach so the insert failing doesn't block the upload.
    try {
      await supabase.from("user_documents").insert({
        user_id: userId,
        submission_id: submissionId || null,
        name: newObj.name || file.name,
        bucket: ONBOARDING_DOCS_BUCKET,
        path: filePath,
        file_type: file.type,
        file_size: file.size,
      });
    } catch (insertErr) {
      console.error("Failed to record uploaded document:", insertErr);
    }
    // Replace with metadata object that will be stored in the submission JSON
    return {
      name: newObj.name || file.name,
      type: file.type,
      size: file.size,
      path: filePath,
    };
  }
  // If object already has name and path, treat as existing file metadata
  if (typeof newObj.name === "string" && newObj.path) {
    return {
      name: newObj.name,
      type: newObj.type,
      size: newObj.size,
      path: newObj.path,
    };
  }
  // Recursively process nested properties
  for (const key in newObj) {
    if (Object.prototype.hasOwnProperty.call(newObj, key)) {
      newObj[key] = await processFilesForUpload(
        newObj[key],
        userId,
        submissionId,
      );
    }
  }
  return newObj;
};

const dataUrlToBlob = async (dataUrl: string) => {
  const res = await fetch(dataUrl);
  return await res.blob();
};

const toSnakeCase = (data: any): any => {
  if (Array.isArray(data)) {
    return data.map((item) => toSnakeCase(item));
  }
  if (
    data !== null && typeof data === "object" && !(data instanceof Date) &&
    !(data instanceof File)
  ) {
    const snaked: Record<string, any> = {};
    for (const key in data) {
      if (Object.prototype.hasOwnProperty.call(data, key)) {
        const snakeKey = key.replace(
          /[A-Z]/g,
          (letter) => `_${letter.toLowerCase()}`,
        );
        snaked[snakeKey] = toSnakeCase(data[key]);
      }
    }
    return snaked;
  }
  return data;
};

const toCamelCase = (data: any): any => {
  if (Array.isArray(data)) {
    return data.map((item) => toCamelCase(item));
  }
  if (data !== null && typeof data === "object" && !(data instanceof Date)) {
    const camelCased: Record<string, any> = {};
    for (const key in data) {
      if (Object.prototype.hasOwnProperty.call(data, key)) {
        const camelKey = key.replace(/_([a-z])/g, (g) => g[1].toUpperCase());
        camelCased[camelKey] = toCamelCase(data[key]);
      }
    }
    return processUrlsForDisplay(camelCased);
  }
  return data;
};

// Helper for fetching large datasets through pagination (Supabase default limit is 1000)
const fetchAllPaginated = async (
  queryBase: any,
  maxRows: number = 10000,
): Promise<any[]> => {
  let allData: any[] = [];
  let from = 0;
  const step = 1000;
  let hasMore = true;

  while (hasMore && allData.length < maxRows) {
    // Clone the query base to avoid side effects if reused
    const { data, error } = await queryBase
      .range(from, from + step - 1);

    if (error) throw error;
    if (!data || data.length === 0) {
      hasMore = false;
    } else {
      allData = [...allData, ...data];
      if (data.length < step) {
        hasMore = false;
      }
    }
    from += step;
  }
  return allData;
};

export const api = {
  // --- Initial Data Loading ---
  getInitialAppData: async (): Promise<
    { settings: any; roles: Role[]; holidays: Holiday[] }
  > => {
    const { data: settingsData, error: settingsError } = await supabase
      .from("settings")
      .select("*")
      .eq("id", "singleton")
      .single();
    if (settingsError) {
      throw new Error("Failed to fetch core application settings.");
    }

    const rolesQuery = supabase.from("roles").select("*");
    const rolesData = await fetchAllPaginated(rolesQuery);

    const holidaysQuery = supabase.from("holidays").select("*");
    const holidaysData = await fetchAllPaginated(holidaysQuery);

    return {
      settings: toCamelCase(settingsData),
      roles: (rolesData || []).map(toCamelCase),
      holidays: (holidaysData || []).map(toCamelCase),
    };
  },

  updateSettings: async (settings: any): Promise<void> => {
    const { error } = await supabase
      .from("settings")
      .update(toSnakeCase(settings))
      .eq("id", "singleton");
    if (error) throw error;
  },

  // --- Onboarding & Verification ---
  getVerificationSubmissions: async (
    status?: string,
    organizationId?: string,
  ): Promise<OnboardingData[]> => {
    let query = supabase.from("onboarding_submissions").select("*");
    if (status) query = query.eq("status", status);
    if (organizationId) query = query.eq("organization_id", organizationId);
    query = query.order("created_at", {
      ascending: false,
    });
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },

  getOnboardingDataById: async (id: string): Promise<OnboardingData | null> => {
    const { data, error } = await supabase.from("onboarding_submissions")
      .select("*").eq("id", id).single();
    if (error && error.code !== "PGRST116") throw error;
    return data ? toCamelCase(data) : null;
  },

  _saveSubmission: async (
    data: OnboardingData,
    asDraft: boolean,
  ): Promise<{ draftId: string }> => {
    const { data: { session } } = await supabase.auth.getSession();
    if (!session?.user) throw new Error("User not authenticated");

    const submissionId = (data.id && !data.id.startsWith("draft_"))
      ? data.id
      : crypto.randomUUID();
    const dataWithPaths = await processFilesForUpload(
      data,
      session.user.id,
      submissionId,
    );

    const dbData = {
      ...toSnakeCase(dataWithPaths),
      id: submissionId,
      user_id: session.user.id,
      status: asDraft ? "draft" : data.status,
    };

    delete dbData.file;
    delete dbData.confirm_account_number;

    const { data: savedData, error } = await supabase.from(
      "onboarding_submissions",
    ).upsert(dbData, { onConflict: "id" }).select("id").single();
    if (error) throw error;
    return { draftId: savedData.id };
  },

  saveDraft: async (data: OnboardingData) => api._saveSubmission(data, true),

  submitOnboarding: async (data: OnboardingData) => {
    const { draftId } = await api._saveSubmission(data, false);
    const submittedData = await api.getOnboardingDataById(draftId);
    if (!submittedData) throw new Error("Failed to retrieve submitted data.");
    return submittedData;
  },

  updateOnboarding: async (data: OnboardingData) => {
    const { draftId } = await api._saveSubmission(
      data,
      data.status === "draft",
    );
    const updatedData = await api.getOnboardingDataById(draftId);
    if (!updatedData) throw new Error("Failed to retrieve updated data.");
    return updatedData;
  },

  verifySubmission: async (id: string): Promise<void> => {
    const { error } = await supabase.from("onboarding_submissions").update({
      status: "verified",
      portal_sync_status: "pending_sync",
    }).eq("id", id);
    if (error) throw error;
  },

  requestChanges: async (id: string, reason: string): Promise<void> => {
    const { error } = await supabase.from("onboarding_submissions").update({
      status: "rejected",
    }).eq("id", id);
    if (error) throw error;
  },

  syncPortals: async (id: string): Promise<OnboardingData> => {
    const { data, error } = await supabase.functions.invoke("sync-portals", {
      body: { submissionId: id },
    });
    if (error) throw error;
    return toCamelCase(data);
  },

  deleteFile: async (filePath: string): Promise<void> => {
    const { error } = await supabase.storage.from(ONBOARDING_DOCS_BUCKET)
      .remove([filePath]);
    if (error) throw error;
  },

  /**
   * Upload a single file to Supabase Storage.  The default bucket is
   * `ONBOARDING_DOCS_BUCKET`, but a different bucket can be specified if
   * necessary (for example when uploading attachments for tasks or support
   * tickets).  After uploading the file, the public URL is returned along
   * with the storage path.  This helper will also create an entry in
   * the `user_documents` table recording the upload, allowing CRUD
   * operations on uploaded documents through the database.  The
   * `submissionId` and `docName` parameters are optional; when provided
   * they are stored in the user_documents table for additional context.
   *
   * @param file     The File object to upload
   * @param bucket   The storage bucket to upload to
   * @param submissionId (optional) ID of the related onboarding submission
   * @param docName  (optional) descriptive name of the document (e.g. 'idProofFront')
   */
  uploadDocument: async (
    file: File,
    bucket: string = ONBOARDING_DOCS_BUCKET,
    submissionId?: string,
    docName?: string,
  ): Promise<{ url: string; path: string }> => {
    const { data: { session } } = await supabase.auth.getSession();
    const userId = session?.user?.id || "anonymous_user";
    const filePath = `${userId}/documents/${Date.now()}_${file.name}`;
    const { error: uploadError } = await supabase.storage.from(bucket).upload(
      filePath,
      file,
    );
    if (uploadError) throw uploadError;
    const { data } = supabase.storage.from(bucket).getPublicUrl(filePath);
    if (!data.publicUrl) {
      throw new Error("Could not get public URL for uploaded file.");
    }

    // Record the uploaded document in the user_documents table.  Use a try/catch
    // so that even if this insert fails, the upload will still succeed.
    try {
      await supabase.from("user_documents").insert({
        user_id: userId,
        submission_id: submissionId || null,
        name: docName || file.name,
        bucket,
        path: filePath,
        file_type: file.type,
        file_size: file.size,
      });
    } catch (e) {
      console.error("Failed to insert user_documents record:", e);
    }
    return { url: data.publicUrl, path: filePath };
  },

  // --- Users & Orgs ---
  getUsers: async (): Promise<User[]> => {
    const query = supabase.from("users").select("*, role_id");
    const data = await fetchAllPaginated(query);
    return (data || []).map((u) => toCamelCase({ ...u, role: u.role_id }));
  },
  getUsersWithManagers: async (): Promise<
    (User & { managerName?: string })[]
  > => {
    const query = supabase.from("users").select(
      "*, reporting_manager_id, role_id",
    );
    const users = await fetchAllPaginated(query);
    const camelUsers = (users || []).map((u) =>
      toCamelCase({ ...u, role: u.role_id })
    );
    const userMap = new Map(camelUsers.map((u) => [u.id, u.name]));
    return camelUsers.map((u) => ({
      ...u,
      managerName: u.reportingManagerId
        ? userMap.get(u.reportingManagerId)
        : undefined,
    }));
  },
  getFieldOfficers: async () =>
    api.getUsers().then((users) =>
      users.filter((u) => u.role === "field_officer")
    ),
  /**
   * Fetch users who should receive check‑in/out notifications.
   * Includes HR, operations managers, admins, developers and site managers.
   * This allows the app to notify the appropriate stakeholders when a
   * field officer checks in or out.
   */
  getNearbyUsers: async () => {
    const allUsers = await api.getUsers();
    // Determine availability based on today's attendance events.  We fetch
    // all events from the beginning of the current day until now and then
    // look at the most recent event per user.  If the latest event is a
    // check‑in (type === 'check in' or 'check-in'), the user is marked
    // available; otherwise, unavailable.  Errors while fetching events
    // will be silently ignored and all users will default to available.
    let availability: Record<string, boolean> = {};
    try {
      const start = new Date();
      start.setHours(0, 0, 0, 0);
      const end = new Date();
      const events = await api.getAllAttendanceEvents(
        start.toISOString(),
        end.toISOString(),
      );
      const latest: Record<string, { type: string; timestamp: string }> = {};
      events.forEach((evt) => {
        const { userId, type, timestamp } = evt as any;
        if (!userId) return;
        if (
          !latest[userId] ||
          new Date(timestamp) > new Date(latest[userId].timestamp)
        ) {
          latest[userId] = { type: type.toLowerCase(), timestamp };
        }
      });
      availability = Object.fromEntries(
        Object.entries(latest).map(([userId, info]) => {
          // Normalize the type string: remove underscores, hyphens, convert to lowercase
          const normalizedType = info.type.toLowerCase().replace(/[-_\s]/g, "");
          // User is available only if their latest event is check-in
          const isCheckIn = normalizedType === "checkin";
          console.log(
            `User ${userId}: latest event type = "${info.type}", normalized = "${normalizedType}", isCheckIn = ${isCheckIn}`,
          );
          return [userId, isCheckIn];
        }),
      );
    } catch (e) {
      console.warn("Failed to compute user availability:", e);
    }
    return allUsers
      .map((u) => ({ ...u, isAvailable: availability[u.id] ?? false }));
  },

  updateUser: async (id: string, updates: Partial<User>) => {
    const { role, ...rest } = updates;
    const dbUpdates: any = toSnakeCase(rest);
    if (role) dbUpdates.role_id = role;

    if ("photo_url" in dbUpdates) {
      const { data: { session } } = await supabase.auth.getSession();
      if (!session?.user) {
        throw new Error("User not authenticated for photo upload");
      }
      const userId = session.user.id;

      const { data: currentUserData } = await supabase.from("users").select(
        "photo_url",
      ).eq("id", id).single();
      const oldPhotoUrl = currentUserData?.photo_url;

      const deleteOldAvatar = async (oldUrl: string | null | undefined) => {
        if (!oldUrl) return;
        try {
          const urlObject = new URL(oldUrl);
          const pathWithBucket = urlObject.pathname.split("/public/")[1];
          if (pathWithBucket) {
            const [bucketName, ...pathParts] = pathWithBucket.split("/");
            const oldPath = pathParts.join("/");
            if (oldPath) {
              await supabase.storage.from(bucketName).remove([oldPath]);
            }
          }
        } catch (e) {
          console.error("Failed to process old avatar URL for deletion:", e);
        }
      };

      if (dbUpdates.photo_url && dbUpdates.photo_url.startsWith("data:")) {
        await deleteOldAvatar(oldPhotoUrl);
        const blob = await dataUrlToBlob(dbUpdates.photo_url);
        const fileExt = dbUpdates.photo_url.split(";")[0].split("/")[1] ||
          "jpg";
        const filePath = `${userId}/${Date.now()}.${fileExt}`;
        // Allow overwriting existing files if the same filename is used by enabling the `upsert` option.
        // Without `upsert: true`, Supabase will reject uploads with duplicate paths.
        const { error: uploadError } = await supabase.storage.from(
          AVATAR_BUCKET,
        ).upload(filePath, blob, { upsert: true });
        if (uploadError) throw uploadError;
        const { data: { publicUrl } } = supabase.storage.from(AVATAR_BUCKET)
          .getPublicUrl(filePath);
        dbUpdates.photo_url = publicUrl;
      } else if (dbUpdates.photo_url === null) {
        await deleteOldAvatar(oldPhotoUrl);
      }
    }

    const { data, error } = await supabase.from("users").update(dbUpdates).eq(
      "id",
      id,
    ).select().single();
    if (error) throw error;
    return toCamelCase({ ...data, role: data.role_id });
  },

  adminChangePassword: async (
    userId: string,
    newPassword: string,
  ): Promise<void> => {
    const { error } = await supabase.functions.invoke("admin-manage-user", {
      body: { action: "change-password", userId, password: newPassword },
    });
    if (error) throw error;
  },

  adminResetPassword: async (userId: string, email: string): Promise<void> => {
    const { error } = await supabase.functions.invoke("admin-manage-user", {
      body: { action: "reset-password", userId, email },
    });
    if (error) throw error;
  },

  createUser: async (userData: Partial<User>): Promise<User> => {
    const { role, ...rest } = userData;
    const dbData: any = toSnakeCase(rest);
    if (role) dbData.role_id = role;

    const { data, error } = await supabase.from("users").insert(dbData).select()
      .single();
    if (error) throw error;
    return toCamelCase({ ...data, role: data.role_id });
  },

  deleteUser: async (id: string) => {
    // Helper to safely execute delete without throwing on missing tables or secondary errors
    const safeDelete = async (operation: any, tableName: string) => {
      try {
        const { error } = await operation;
        if (error && error.code !== "PGRST204" && error.status !== 404) {
          console.warn(`Warning during ${tableName} cleanup:`, error);
        }
      } catch (error: any) {
        // Log warnings for debugging but don't block the main deletion
        if (error?.code !== "PGRST204" && error?.status !== 404) {
          console.warn(`Critical error during ${tableName} cleanup:`, error);
        }
      }
    };

    // 1. Update any users who report to this user - set reporting_manager_id to null
    await safeDelete(
      supabase.from("users").update({
        reporting_manager_id: null,
      }).eq("reporting_manager_id", id),
      "users (reporting manager)",
    );

    // 2. Delete from dependent tables to avoid FK conflicts
    // We execute these deletions to wipe the user's data footprint.

    // Onboarding & Profile
    await safeDelete(
      supabase.from("onboarding_submissions").delete().eq("user_id", id),
      "onboarding_submissions",
    );

    // Locations & Tracking
    await safeDelete(
      supabase.from("user_locations").delete().eq("user_id", id),
      "user_locations",
    );
    await safeDelete(
      supabase.from("locations").update({ created_by: null }).eq(
        "created_by",
        id,
      ),
      "locations (creator)",
    );

    // Attendance & Approvals
    await safeDelete(
      supabase.from("attendance_events").delete().eq("user_id", id),
      "attendance_events",
    );
    await safeDelete(
      supabase.from("attendance_approvals").delete().eq("user_id", id),
      "attendance_approvals (requester)",
    );
    await safeDelete(
      supabase.from("attendance_approvals").delete().eq("manager_id", id),
      "attendance_approvals (manager)",
    );

    // Leave & Comp-Off
    await safeDelete(
      supabase.from("leave_requests").delete().eq("user_id", id),
      "leave_requests",
    );
    await safeDelete(
      supabase.from("leave_requests").update({ current_approver_id: null }).eq(
        "current_approver_id",
        id,
      ),
      "leave_requests (approver)",
    );
    await safeDelete(
      supabase.from("comp_off_logs").delete().eq("user_id", id),
      "comp_off_logs",
    );
    await safeDelete(
      supabase.from("comp_off_logs").update({ granted_by_id: null }).eq(
        "granted_by_id",
        id,
      ),
      "comp_off_logs (granter)",
    );
    await safeDelete(
      supabase.from("extra_work_logs").delete().eq("user_id", id),
      "extra_work_logs",
    );
    await safeDelete(
      supabase.from("extra_work_logs").update({ approver_id: null }).eq(
        "approver_id",
        id,
      ),
      "extra_work_logs (approver)",
    );

    // Tasks & Notifications
    await safeDelete(
      supabase.from("notifications").delete().eq("user_id", id),
      "notifications",
    );
    await safeDelete(
      supabase.from("tasks").delete().eq("assigned_to_id", id),
      "tasks (assigned)",
    );
    await safeDelete(
      supabase.from("tasks").update({ created_by_id: null }).eq(
        "created_by_id",
        id,
      ),
      "tasks (creator)",
    );
    await safeDelete(
      supabase.from("tasks").update({ escalation_level1_user_id: null }).eq(
        "escalation_level1_user_id",
        id,
      ),
      "tasks (escalation L1)",
    );
    await safeDelete(
      supabase.from("tasks").update({ escalation_level2_user_id: null }).eq(
        "escalation_level2_user_id",
        id,
      ),
      "tasks (escalation L2)",
    );

    // Support Desk
    await safeDelete(
      supabase.from("support_tickets").update({ raised_by_id: null }).eq(
        "raised_by_id",
        id,
      ),
      "support_tickets (raised by)",
    );
    await safeDelete(
      supabase.from("support_tickets").update({ assigned_to_id: null }).eq(
        "assigned_to_id",
        id,
      ),
      "support_tickets (assigned to)",
    );
    await safeDelete(
      supabase.from("ticket_posts").delete().eq("author_id", id),
      "ticket_posts",
    );
    await safeDelete(
      supabase.from("ticket_comments").delete().eq("author_id", id),
      "ticket_comments",
    );

    // Uniform Requests
    await safeDelete(
      supabase.from("uniform_requests").update({ requested_by_id: null }).eq(
        "requested_by_id",
        id,
      ),
      "uniform_requests",
    );

    // Patrolling
    await safeDelete(
      supabase.from("patrol_logs").delete().eq("user_id", id),
      "patrol_logs",
    );
    await safeDelete(
      supabase.from("patrol_daily_scores").delete().eq("user_id", id),
      "patrol_daily_scores",
    );
    await safeDelete(
      supabase.from("patrol_qr_codes").update({ created_by: null }).eq(
        "created_by",
        id,
      ),
      "patrol_qr_codes",
    );

    // 3. Finally delete the user from Supabase Auth (which cascades to public.users)
    try {
      const { error: fnError } = await supabase.functions.invoke(
        "admin-manage-user",
        {
          body: { action: "delete-user", userId: id },
        },
      );
      if (fnError) throw fnError;
    } catch (e) {
      console.warn(
        "Edge function delete-user failed, falling back to direct public.users deletion:",
        e,
      );
      const { error } = await supabase.from("users").delete().eq("id", id);
      if (error) throw error;
    }
  },

  updateUserReportingManager: async (
    userId: string,
    managerId: string | null,
  ) => {
    const { error } = await supabase.from("users").update({
      reporting_manager_id: managerId,
    }).eq("id", userId);
    if (error) throw error;
  },

  getOrganizations: async (): Promise<Organization[]> => {
    const query = supabase.from("organizations").select("*")
      .order("short_name");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  createOrganization: async (org: Organization): Promise<Organization> => {
    const { data, error } = await supabase.from("organizations").insert(
      toSnakeCase(org),
    ).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  getOrganizationStructure: async (): Promise<OrganizationGroup[]> => {
    const { data: groups, error: groupsError } = await supabase.from(
      "organization_groups",
    ).select("*");
    if (groupsError) throw groupsError;
    const { data: companies, error: companiesError } = await supabase.from(
      "companies",
    ).select("*");
    if (companiesError) throw companiesError;
    const { data: entities, error: entitiesError } = await supabase.from(
      "entities",
    ).select("*");
    if (entitiesError) throw entitiesError;

    const camelGroups: any[] = (groups || []).map(toCamelCase);
    const camelCompanies: any[] = (companies || []).map(toCamelCase);
    const camelEntities: any[] = (entities || []).map(toCamelCase);

    const companyMap = new Map<string, any[]>();
    camelCompanies.forEach((company) => {
      const companyWithEntities = {
        ...company,
        entities: camelEntities.filter((e) => e.companyId === company.id),
      };
      if (!companyMap.has(company.groupId)) companyMap.set(company.groupId, []);
      companyMap.get(company.groupId)!.push(companyWithEntities);
    });

    return camelGroups.map((group) => ({
      ...group,
      companies: companyMap.get(group.id) || [],
      locations: [],
    }));
  },
  getSiteConfigurations: async (): Promise<SiteConfiguration[]> => {
    const query = supabase.from("site_configurations").select(
      "*",
    );
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  bulkUploadOrganizations: async (
    orgs: Organization[],
  ): Promise<{ count: number }> => {
    const { count, error } = await supabase.from("organizations").upsert(
      toSnakeCase(orgs),
      { onConflict: "id" },
    );
    if (error) throw error;
    return { count: count || 0 };
  },
  getModules: async (): Promise<AppModule[]> => {
    const query = supabase.from("app_modules").select("*");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  saveModules: async (modules: AppModule[]): Promise<void> => {
    // Ensure timestamps are present
    const modulesWithTimestamps = modules.map((m) => ({
      ...m,
      created_at: m.created_at || new Date().toISOString(),
      updated_at: new Date().toISOString(),
    }));

    const { error } = await supabase.from("app_modules").upsert(
      toSnakeCase(modulesWithTimestamps),
    );
    if (error) throw error;
  },
  getRoles: async (): Promise<Role[]> => {
    const query = supabase.from("roles").select("*");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  saveRoles: async (roles: Role[]): Promise<void> => {
    const { error } = await supabase.from("roles").upsert(toSnakeCase(roles));
    if (error) throw error;
  },
  getHolidays: async (): Promise<Holiday[]> => {
    const query = supabase.from("holidays").select("*");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  addHoliday: async (holiday: Omit<Holiday, "id">): Promise<Holiday> => {
    const { data, error } = await supabase.from("holidays").insert(
      toSnakeCase(holiday),
    ).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  removeHoliday: async (id: string): Promise<void> => {
    const { error } = await supabase.from("holidays").delete().eq("id", id);
    if (error) throw error;
  },
  getAttendanceEvents: async (
    userId: string,
    start: string,
    end: string,
  ): Promise<AttendanceEvent[]> => {
    const query = supabase.from("attendance_events").select("*")
      .eq("user_id", userId).gte("timestamp", start).lte("timestamp", end);
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  getAllAttendanceEvents: async (
    start: string,
    end: string,
  ): Promise<AttendanceEvent[]> => {
    const query = supabase.from("attendance_events").select("*")
      .gte("timestamp", start).lte("timestamp", end);
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  getAttendanceDashboardData: async (
    startDate: Date,
    endDate: Date,
    currentDate: Date,
    timezone: string = "UTC",
  ) => {
    const { data, error } = await supabase.rpc(
      "get_attendance_dashboard_data",
      {
        start_date_iso: format(startDate, "yyyy-MM-dd"),
        end_date_iso: format(endDate, "yyyy-MM-dd"),
        current_date_iso: format(currentDate, "yyyy-MM-dd"),
        p_timezone: timezone,
      },
    );
    if (error) {
      throw new Error(
        "Could not load attendance dashboard data from the database function.",
      );
    }
    return data;
  },
  /**
   * Insert an attendance event.  The event may optionally include a
   * `locationId` if the user checked in/out within a geofenced location.
   * The caller should send latitude/longitude regardless of whether a
   * location was detected; this preserves the original coordinates for
   * auditing and reverse geocoding.
   *
   * @param event An attendance event without an id.  Accepts optional
   *              latitude, longitude and locationId properties.
   */
  addAttendanceEvent: async (
    event: Omit<AttendanceEvent, "id">,
  ): Promise<void> => {
    const { error } = await supabase
      .from("attendance_events")
      .insert(toSnakeCase(event));
    if (error) throw error;
  },

  /**
   * Retrieve all geofenced locations.  Returns an array of Location
   * objects with camelCased keys.
   */
  /**
   * Retrieve all defined geofenced locations.  In addition to the base
   * fields stored on the locations table, this query joins the
   * `users` table to fetch the name of the user that created the
   * location.  The resulting records include a `createdByName` field
   * which will be undefined if no creator is stored or the creator
   * record is not found.  This helper returns camel‑cased keys.
   */
  getLocations: async (): Promise<Location[]> => {
    // Join the locations table to the users table via the created_by
    // foreign key.  Alias the joined user as created_by_user so the
    // resulting object includes nested fields under that key.  We
    // explicitly select all location columns, then the name of the
    // creator.  Supabase will return an array of objects with a
    // created_by_user property containing the joined user row.
    const query = supabase
      .from("locations")
      .select("*, created_by_user:created_by (id, name)");
    const data = await fetchAllPaginated(query);
    // Convert to camelCase and hoist the creator name into
    // createdByName for convenience.  Preserve other fields as is.
    return (data || []).map((raw: any) => {
      const camel = toCamelCase(raw) as any;
      const createdByUser = camel.createdByUser as {
        id?: string;
        name?: string;
      } | undefined;
      const createdByName = createdByUser?.name || undefined;
      // Remove the nested createdByUser field to avoid leaking
      // implementation details.  Spread the camel object first to
      // preserve all other properties.
      const { createdByUser: _omit, ...rest } = camel;
      return { ...rest, createdByName } as Location;
    });
  },

  /**
   * Retrieve locations assigned to a user.  This joins user_locations
   * and locations to return full Location records.  Only assignments
   * where user_id matches are returned (enforced by RLS).  Admins can
   * override by specifying a different userId.
   */
  getUserLocations: async (userId: string): Promise<Location[]> => {
    const query = supabase
      .from("user_locations")
      .select("location_id:location_id (*), id")
      .eq("user_id", userId);
    const data = await fetchAllPaginated(query);
    // Flatten nested location record from join: { location_id: { ... } }
    return (data || []).map((row: any) => {
      const loc = row.location_id || {};
      return toCamelCase(loc);
    }) as Location[];
  },

  /**
   * Create a new geofenced location.  Returns the inserted Location
   * record.  The caller must provide name, latitude, longitude and
   * radius.  createdBy is optional and will be stored if provided.
   */
  createLocation: async (loc: {
    name?: string | null;
    latitude: number;
    longitude: number;
    radius: number;
    /** Optional pre‑computed address for this location */
    address?: string | null;
    createdBy?: string | null;
  }): Promise<Location> => {
    // Convert camelCased keys to snake_cased for Supabase
    const payload = toSnakeCase(loc);
    const { data, error } = await supabase
      .from("locations")
      .insert(payload)
      .select("*")
      .single();
    if (error) throw error;
    return toCamelCase(data) as Location;
  },

  /**
   * Assign an existing location to a user.  This creates a row in
   * user_locations.  Only the id of the new assignment is returned.
   */
  assignLocationToUser: async (
    userId: string,
    locationId: string,
  ): Promise<void> => {
    const { error } = await supabase
      .from("user_locations")
      .insert({ user_id: userId, location_id: locationId });
    if (error) throw error;
  },

  /**
   * Update an existing location by ID.  Accepts a partial set of
   * fields (name, latitude, longitude, radius, address).  Returns the
   * updated Location record.
   */
  updateLocation: async (id: string, updates: {
    name?: string | null;
    latitude?: number;
    longitude?: number;
    radius?: number;
    address?: string | null;
  }): Promise<Location> => {
    const payload = toSnakeCase(updates);
    const { data, error } = await supabase
      .from("locations")
      .update(payload)
      .eq("id", id)
      .select("*")
      .single();
    if (error) throw error;
    return toCamelCase(data) as Location;
  },

  /**
   * Delete a location by ID.  Cascading deletes remove any
   * user_locations referencing this location.  Returns void.
   */
  deleteLocation: async (id: string): Promise<void> => {
    const { error } = await supabase
      .from("locations")
      .delete()
      .eq("id", id);
    if (error) throw error;
  },

  /**
   * Remove a user->location assignment.  Deletes the row from
   * user_locations for the given userId and locationId.
   */
  unassignLocationFromUser: async (
    userId: string,
    locationId: string,
  ): Promise<void> => {
    const { error } = await supabase
      .from("user_locations")
      .delete()
      .match({ user_id: userId, location_id: locationId });
    if (error) throw error;
  },

  /**
   * Upload a company logo to the dedicated `logo` bucket.  Returns the
   * public URL of the uploaded image.  The caller should persist
   * this URL in the application settings so it can be used as the
   * active or default logo.  A unique filename is generated to avoid
   * overwriting existing objects.
   */
  uploadLogo: async (file: File): Promise<string> => {
    const fileExt = file.name.split(".").pop() || "png";
    const fileName = `${Date.now()}_${file.name}`;
    const filePath = `${fileName}`;
    const { error: uploadError } = await supabase.storage.from(LOGO_BUCKET)
      .upload(filePath, file, { upsert: true });
    if (uploadError) throw uploadError;
    const { data: { publicUrl } } = supabase.storage.from(LOGO_BUCKET)
      .getPublicUrl(filePath);
    return publicUrl;
  },

  /**
   * Fetch all logos stored in the `logo` bucket.  Returns an array of
   * objects with `name` and `url` for each file.  This is useful for
   * listing available logos in the interface settings modal.
   */
  getLogos: async (): Promise<{ name: string; url: string }[]> => {
    const { data, error } = await supabase.storage.from(LOGO_BUCKET).list("", {
      limit: 100,
      sortBy: { column: "name", order: "asc" },
    });
    if (error) throw error;
    return (data || []).map((obj) => {
      const { data: { publicUrl } } = supabase.storage.from(LOGO_BUCKET)
        .getPublicUrl(obj.name);
      return { name: obj.name, url: publicUrl };
    });
  },

  /**
   * Delete a logo from the `logo` bucket by its filename.  Returns void
   * on success.  The caller should remove any references to this logo
   * in settings.  If the file does not exist, this call is a no-op.
   */
  deleteLogo: async (fileName: string): Promise<void> => {
    const { error } = await supabase.storage.from(LOGO_BUCKET).remove([
      fileName,
    ]);
    if (error) throw error;
  },

  /**
   * Upload a background image to the `background` bucket for use in the
   * login carousel.  Returns the public URL of the uploaded image.
   */
  uploadBackground: async (file: File): Promise<string> => {
    const fileExt = file.name.split(".").pop() || "png";
    const fileName = `${Date.now()}_${file.name}`;
    const filePath = `${fileName}`;
    const { error: uploadError } = await supabase.storage.from(
      BACKGROUND_BUCKET,
    ).upload(filePath, file, { upsert: true });
    if (uploadError) throw uploadError;
    const { data: { publicUrl } } = supabase.storage.from(BACKGROUND_BUCKET)
      .getPublicUrl(filePath);
    return publicUrl;
  },

  /**
   * Fetch all background images from the `background` bucket.  Returns
   * an array of objects with `name` and `url` for each file.
   */
  getBackgroundImages: async (): Promise<{ name: string; url: string }[]> => {
    const { data, error } = await supabase.storage.from(BACKGROUND_BUCKET).list(
      "",
      { limit: 100, sortBy: { column: "name", order: "asc" } },
    );
    if (error) throw error;
    return (data || []).map((obj) => {
      const { data: { publicUrl } } = supabase.storage.from(BACKGROUND_BUCKET)
        .getPublicUrl(obj.name);
      return { name: obj.name, url: publicUrl };
    });
  },

  /**
   * Delete a background image by filename from the `background` bucket.
   */
  deleteBackground: async (fileName: string): Promise<void> => {
    const { error } = await supabase.storage.from(BACKGROUND_BUCKET).remove([
      fileName,
    ]);
    if (error) throw error;
  },
  getAttendanceSettings: async (): Promise<AttendanceSettings> => {
    const { data, error } = await supabase.from("settings").select(
      "attendance_settings",
    ).eq("id", "singleton").single();
    if (error) throw error;
    if (!data?.attendance_settings) {
      throw new Error("Attendance settings are not configured.");
    }
    return toCamelCase(data.attendance_settings) as AttendanceSettings;
  },
  saveAttendanceSettings: async (
    settings: AttendanceSettings,
  ): Promise<void> => {
    // Ensure we are upserting with the correct ID and data structure
    const { error } = await supabase
      .from("settings")
      .upsert(
        {
          id: "singleton",
          attendance_settings: toSnakeCase(settings),
          updated_at: new Date().toISOString(),
        },
        { onConflict: "id" },
      );

    if (error) {
      console.error("Error saving attendance settings:", error);
      throw error;
    }
  },

  getRecurringHolidays: async (): Promise<RecurringHolidayRule[]> => {
    const query = supabase.from("recurring_holidays").select("*");
    const data = await fetchAllPaginated(query);
    return (data || []).map((row) => ({
      id: row.id,
      type: row.role_type,
      day: row.day,
      n: row.occurrence,
    }));
  },

  addRecurringHoliday: async (
    rule: RecurringHolidayRule,
  ): Promise<RecurringHolidayRule> => {
    const dbRule = {
      role_type: rule.type || "office",
      day: rule.day,
      occurrence: rule.n,
    };
    const { data, error } = await supabase.from("recurring_holidays").insert(
      dbRule,
    ).select().single();
    if (error) throw error;
    return {
      id: data.id,
      type: data.role_type,
      day: data.day,
      n: data.occurrence,
    };
  },

  deleteRecurringHoliday: async (id: string): Promise<void> => {
    const { error } = await supabase.from("recurring_holidays").delete().eq(
      "id",
      id,
    );
    if (error) throw error;
  },
  createAssignment: async (
    officerId: string,
    siteId: string,
    date: string,
  ): Promise<void> => {
    const site = (await api.getOrganizations()).find((o) => o.id === siteId);
    await api.createTask({
      name: `Visit ${site?.shortName || "site"} for verification`,
      description:
        `Perform on-site duties and verification tasks for ${site?.shortName}.`,
      dueDate: date,
      priority: "Medium",
      assignedToId: officerId,
    });
  },
  getLeaveBalancesForUser: async (userId: string): Promise<LeaveBalance> => {
    const getStaffType = (
      role: UserRole,
    ):
      | "office"
      | "field" => (["hr", "admin", "finance"].includes(role)
        ? "office"
        : "field");
    const { data: settingsData, error: settingsError } = await supabase.from(
      "settings",
    ).select("attendance_settings").eq("id", "singleton").single();
    if (settingsError || !settingsData?.attendance_settings) {
      throw new Error("Could not load attendance rules.");
    }
    const { data: userData, error: userError } = await supabase.from("users")
      .select("role_id").eq("id", userId).single();
    if (userError) throw userError;

    const staffType = getStaffType(userData.role_id);
    const rules =
      (toCamelCase(settingsData.attendance_settings) as AttendanceSettings)[
        staffType
      ];

    const [
      { data: approvedLeaves, error: leavesError },
      { data: compOffData, error: compOffError },
      { data: otData, error: otError },
    ] = await Promise.all([
      supabase.from("leave_requests").select(
        "leave_type, start_date, end_date, day_option",
      ).eq("user_id", userId).eq("status", "approved"),
      supabase.from("comp_off_logs").select("*").eq("user_id", userId),
      supabase.from("extra_work_logs").select("hours_worked").eq(
        "user_id",
        userId,
      ).eq("claim_type", "OT").eq("status", "Approved").gte(
        "work_date",
        format(startOfMonth(new Date()), "yyyy-MM-dd"),
      ).lte("work_date", format(endOfMonth(new Date()), "yyyy-MM-dd")),
    ]);

    if (leavesError || compOffError || otError) {
      throw new Error("Failed to fetch all leave balance data.");
    }

    const balance: LeaveBalance = {
      userId,
      earnedTotal: rules.annualEarnedLeaves || 0,
      earnedUsed: 0,
      sickTotal: rules.annualSickLeaves || 0,
      sickUsed: 0,
      floatingTotal: (rules.monthlyFloatingLeaves || 0) * 12,
      floatingUsed: 0,
      compOffTotal: (compOffData || []).length,
      compOffUsed:
        (compOffData || []).filter((log) => log.status === "used").length,
      otHoursThisMonth: (otData || []).reduce(
        (sum, log) => sum + (log.hours_worked || 0),
        0,
      ),
    };

    (approvedLeaves || []).forEach((leave) => {
      const leaveAmount = leave.day_option === "half"
        ? 0.5
        : differenceInCalendarDays(
          new Date(leave.end_date),
          new Date(leave.start_date),
        ) + 1;
      if (leave.leave_type === "Earned") balance.earnedUsed += leaveAmount;
      if (leave.leave_type === "Sick") balance.sickUsed += leaveAmount;
      if (leave.leave_type === "Floating") balance.floatingUsed += leaveAmount;
    });

    return balance;
  },
  submitLeaveRequest: async (
    data: Omit<
      LeaveRequest,
      "id" | "status" | "currentApproverId" | "approvalHistory"
    >,
  ): Promise<LeaveRequest> => {
    const { data: userProfile, error: userError } = await supabase.from("users")
      .select("reporting_manager_id").eq("id", data.userId).single();
    if (userError) throw userError;
    const { data: insertedData, error: insertError } = await supabase.from(
      "leave_requests",
    ).insert({
      ...toSnakeCase(data),
      status: "pending_manager_approval",
      current_approver_id: userProfile.reporting_manager_id || null,
      approval_history: [],
    }).select("*").single();
    if (insertError) throw insertError;
    return toCamelCase(insertedData);
  },
  getLeaveRequests: async (
    filter?: {
      userId?: string;
      userIds?: string[];
      status?: string;
      forApproverId?: string;
      startDate?: string;
      endDate?: string;
    },
  ): Promise<LeaveRequest[]> => {
    let query = supabase.from("leave_requests").select("*");
    if (filter?.userId) query = query.eq("user_id", filter.userId);
    if (filter?.userIds) query = query.in("user_id", filter.userIds);
    if (filter?.status) query = query.eq("status", filter.status);
    if (filter?.forApproverId) {
      query = query.eq("current_approver_id", filter.forApproverId);
    }
    if (filter?.startDate && filter?.endDate) {
      query = query.lte("start_date", filter.endDate).gte(
        "end_date",
        filter.startDate,
      );
    }
    query = query.order("start_date", {
      ascending: false,
    });
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  getTasks: async (): Promise<Task[]> => {
    const query = supabase.from("tasks").select("*").order(
      "created_at",
      { ascending: false },
    );
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  createTask: async (taskData: Partial<Task>): Promise<Task> => {
    const { data, error } = await supabase.from("tasks").insert(
      toSnakeCase({ ...taskData, status: "To Do", escalationStatus: "None" }),
    ).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  updateTask: async (id: string, updates: Partial<Task>): Promise<Task> => {
    // Handle completion photo upload
    if (
      (updates as any)?.completionPhoto && (updates as any).completionPhoto.file
    ) {
      const completion: any = (updates as any).completionPhoto;
      const file: File = completion.file;
      try {
        const { path } = await api.uploadDocument(
          file,
          TASK_ATTACHMENTS_BUCKET,
        );
        // Replace the completionPhoto with a JSON object containing metadata and the path
        (updates as any).completionPhoto = {
          name: completion.name,
          type: completion.type,
          size: completion.size,
          path,
        };
      } catch (uploadError) {
        console.error("Failed to upload task completion photo:", uploadError);
        // Remove the file property to avoid sending File object to database
        delete (updates as any).completionPhoto;
      }
    }
    const { data, error } = await supabase.from("tasks").update(
      toSnakeCase(updates),
    ).eq("id", id).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  deleteTask: async (id: string): Promise<void> => {
    const { error } = await supabase.from("tasks").delete().eq("id", id);
    if (error) throw error;
  },
  runAutomaticEscalations: async (): Promise<
    { updatedTasks: Task[]; newNotifications: Notification[] }
  > => {
    const { data, error } = await supabase.functions.invoke("run-escalations");
    if (error) throw error;
    return toCamelCase(data);
  },
  getNotifications: async (userId: string): Promise<Notification[]> => {
    const query = supabase.from("notifications").select("*").eq(
      "user_id",
      userId,
    ).order("created_at", { ascending: false });
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  createNotification: async (
    data: Omit<Notification, "id" | "createdAt" | "isRead">,
  ): Promise<Notification> => {
    const { data: inserted, error } = await supabase.from("notifications")
      .insert(toSnakeCase(data)).select().single();
    if (error) throw error;
    return toCamelCase(inserted);
  },
  markNotificationAsRead: async (id: string): Promise<void> => {
    await supabase.from("notifications").update({ is_read: true }).eq("id", id);
  },
  markAllNotificationsAsRead: async (userId: string): Promise<void> => {
    await supabase.from("notifications").update({ is_read: true }).eq(
      "user_id",
      userId,
    ).eq("is_read", false);
  },

  /**
   * Create a new user in Supabase Auth and the public users table in one call.
   * This method invokes the `admin-create-user` edge function which uses the
   * service role to create an auth user and then updates the `public.users`
   * table with the supplied role.  A verification email will be sent to the
   * specified address.  After creation, the newly created user record is
   * returned by looking it up via the email address.
   *
   * @param data An object containing the new user's name, email, password and role.
   * @returns The newly created user record.
   */
  createAuthUser: async (
    data: { name: string; email: string; password: string; role: string },
  ): Promise<User> => {
    const { name, email, password, role } = data;
    // Attempt to create the user via the Supabase edge function first.  The
    // edge function uses the service role key on the server to create an auth
    // user and update the public.users table with the specified role.  This
    // is the preferred path in production because it avoids exposing the
    // service role key to the browser.
    try {
      const { error: fnError } = await supabase.functions.invoke(
        "admin-create-user",
        {
          body: { name, email, password, role },
        },
      );
      if (fnError) {
        throw fnError;
      }
      // After creation, fetch the newly created user by email from our users
      // view.  The handle_new_user trigger should have inserted the row and
      // the edge function should have set the role.
      const allUsers = await api.getUsers();
      const newUser = allUsers.find((u) => u.email === email);
      if (newUser) return newUser;
      // If the user cannot be found immediately, fall through to the
      // fallback creation path below.
      console.warn(
        "admin-create-user completed but user not found; falling back to client side signup",
      );
    } catch (fnError: any) {
      // If the edge function call fails entirely (e.g. network error or not
      // deployed), we fall back to client side signup using the anon key.
      const msg = typeof fnError?.message === "string"
        ? fnError.message
        : String(fnError);
      if (
        !msg.toLowerCase().includes("failed to send a request") &&
        !msg.toLowerCase().includes("edge function")
      ) {
        // For other errors (such as email already registered), surface them directly.
        throw fnError;
      }
      console.warn(
        "Edge function not reachable, falling back to client side signup",
      );
      // Create the auth user using the client side anon key.  This will send a
      // confirmation email automatically.  We include the name in user
      // metadata so it is persisted in auth.users.
      const { data: signUpData, error: signUpError } = await supabase.auth
        .signUp({
          email,
          password,
          options: { data: { name } },
        });
      if (signUpError || !signUpData?.user) {
        throw signUpError || new Error("Failed to sign up user");
      }
      const newAuthUser = signUpData.user;
      // Upsert into the public.users table.
      try {
        const { error: profileError } = await supabase.from("users").upsert({
          id: newAuthUser.id,
          name,
          email,
          role_id: role,
        }, { onConflict: "id" });

        if (profileError) {
          console.error("Profile upsert failed during fallback:", profileError);
          // If profile creation failed, we should throw so the UI knows the user isn't fully ready.
          throw new Error(
            `Auth account created but profile setup failed: ${profileError.message}`,
          );
        }
      } catch (err: any) {
        console.error("Error during fallback profile setup:", err);
        throw err;
      }

      // Finally return a user object combining the known fields.
      return {
        id: newAuthUser.id,
        name,
        email,
        role,
        phone: undefined as any,
        organizationId: undefined as any,
        organizationName: undefined as any,
        reportingManagerId: undefined as any,
        photoUrl: undefined as any,
      } as User;
    }

    // Default return if no user found by either path.  This should rarely
    // occur; if it does, we return a minimal user object for consistency.
    return {
      id: "",
      name,
      email,
      role,
      phone: undefined as any,
      organizationId: undefined as any,
      organizationName: undefined as any,
      reportingManagerId: undefined as any,
      photoUrl: undefined as any,
    } as User;
  },
  getPolicies: async (): Promise<Policy[]> => {
    const query = supabase.from("policies").select("*");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  createPolicy: async (data: Omit<Policy, "id">): Promise<Policy> => {
    const { data: inserted, error } = await supabase.from("policies").insert(
      toSnakeCase(data),
    ).select().single();
    if (error) throw error;
    return toCamelCase(inserted);
  },
  getInsurances: async (): Promise<Insurance[]> => {
    const query = supabase.from("insurances").select("*");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  createInsurance: async (data: Omit<Insurance, "id">): Promise<Insurance> => {
    const { data: inserted, error } = await supabase.from("insurances").insert(
      toSnakeCase(data),
    ).select().single();
    if (error) throw error;
    return toCamelCase(inserted);
  },
  getApprovalWorkflowSettings: async (): Promise<
    { finalConfirmationRole: UserRole }
  > => {
    const { data, error } = await supabase.from("settings").select(
      "approval_workflow_settings",
    ).eq("id", "singleton").single();
    if (error) throw error;
    if (!data?.approval_workflow_settings) {
      throw new Error("Approval workflow settings are not configured.");
    }
    return toCamelCase(data.approval_workflow_settings);
  },
  updateApprovalWorkflowSettings: async (role: UserRole): Promise<void> => {
    const { error } = await supabase.from("settings").update({
      approval_workflow_settings: toSnakeCase({ finalConfirmationRole: role }),
    }).eq("id", "singleton");
    if (error) throw error;
  },
  approveLeaveRequest: async (
    id: string,
    approverId: string,
  ): Promise<void> => {
    const { data: request, error: fetchError } = await supabase.from(
      "leave_requests",
    ).select("approval_history").eq("id", id).single();
    if (fetchError) throw fetchError;
    const { finalConfirmationRole } = await api.getApprovalWorkflowSettings();
    const { data: finalApprover } = await supabase.from("users").select("id")
      .eq("role_id", finalConfirmationRole).limit(1).single();
    const { data: approverData, error: nameError } = await supabase.from(
      "users",
    ).select("name").eq("id", approverId).single();
    if (nameError) throw nameError;
    const newHistoryRecord = {
      approver_id: approverId,
      approver_name: approverData.name,
      status: "approved",
      timestamp: new Date().toISOString(),
    };
    const updatedHistory = [
      ...((request.approval_history as any[]) || []),
      newHistoryRecord,
    ];
    const { error } = await supabase.from("leave_requests").update({
      status: "pending_hr_confirmation",
      current_approver_id: finalApprover?.id,
      approval_history: updatedHistory,
    }).eq("id", id);
    if (error) throw error;
  },
  rejectLeaveRequest: async (
    id: string,
    approverId: string,
    reason = "",
  ): Promise<void> => {
    const { data: request, error: fetchError } = await supabase.from(
      "leave_requests",
    ).select("approval_history").eq("id", id).single();
    if (fetchError) throw fetchError;
    const { data: approverData, error: nameError } = await supabase.from(
      "users",
    ).select("name").eq("id", approverId).single();
    if (nameError) throw nameError;
    const newHistoryRecord = {
      approver_id: approverId,
      approver_name: approverData.name,
      status: "rejected",
      timestamp: new Date().toISOString(),
      comments: reason,
    };
    const updatedHistory = [
      ...((request.approval_history as any[]) || []),
      newHistoryRecord,
    ];
    const { error } = await supabase.from("leave_requests").update({
      status: "rejected",
      current_approver_id: null,
      approval_history: updatedHistory,
    }).eq("id", id);
    if (error) throw error;
  },
  confirmLeaveByHR: async (id: string, hrId: string): Promise<void> => {
    const { data: request, error: fetchError } = await supabase.from(
      "leave_requests",
    ).select("leave_type, user_id, approval_history").eq("id", id).single();
    if (fetchError) throw fetchError;
    const { data: approverData, error: nameError } = await supabase.from(
      "users",
    ).select("name").eq("id", hrId).single();
    if (nameError) throw nameError;
    const newHistoryRecord = {
      approver_id: hrId,
      approver_name: approverData.name,
      status: "approved",
      timestamp: new Date().toISOString(),
      comments: "Final approval.",
    };
    const updatedHistory = [
      ...((request.approval_history as any[]) || []),
      newHistoryRecord,
    ];
    const { error } = await supabase.from("leave_requests").update({
      status: "approved",
      current_approver_id: null,
      approval_history: updatedHistory,
    }).eq("id", id);
    if (error) throw error;
    if (request.leave_type === "Comp Off") {
      const { data: availableLog, error: logError } = await supabase.from(
        "comp_off_logs",
      ).select("id").eq("user_id", request.user_id).eq("status", "earned")
        .limit(1).single();
      if (logError && logError.code !== "PGRST116") throw logError;
      if (availableLog) {
        await supabase.from("comp_off_logs").update({
          status: "used",
          leave_request_id: id,
        }).eq("id", availableLog.id);
      }
    }
  },
  submitExtraWorkClaim: async (
    claimData: Omit<ExtraWorkLog, "id" | "createdAt" | "status">,
  ): Promise<void> => {
    const { error } = await supabase.from("extra_work_logs").insert(
      toSnakeCase({ ...claimData, status: "Pending" }),
    );
    if (error) throw error;
  },
  getExtraWorkLogs: async (userId?: string): Promise<ExtraWorkLog[]> => {
    let query = supabase.from("extra_work_logs").select("*");
    if (userId) query = query.eq("user_id", userId);
    else query = query.eq("status", "Pending");
    query = query.order("work_date", {
      ascending: false,
    });
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  approveExtraWorkClaim: async (
    claimId: string,
    approverId: string,
  ): Promise<void> => {
    const { data: approverData, error: nameError } = await supabase.from(
      "users",
    ).select("name").eq("id", approverId).single();
    if (nameError) throw nameError;
    const { data: claim, error: fetchError } = await supabase.from(
      "extra_work_logs",
    ).select("*").eq("id", claimId).single();
    if (fetchError) throw fetchError;
    if (!claim) throw new Error("Claim not found.");
    const { error: updateError } = await supabase.from("extra_work_logs")
      .update({
        status: "Approved",
        approver_id: approverId,
        approver_name: approverData.name,
        approved_at: new Date().toISOString(),
      }).eq("id", claimId);
    if (updateError) throw updateError;
    if (claim.claim_type === "Comp Off") {
      await api.addCompOffLog({
        userId: claim.user_id,
        userName: claim.user_name,
        dateEarned: claim.work_date,
        reason: `Claim approved: ${claim.reason}`,
        status: "earned",
        grantedById: approverId,
        grantedByName: approverData.name,
      });
    }
  },
  rejectExtraWorkClaim: async (
    claimId: string,
    approverId: string,
    reason: string,
  ): Promise<void> => {
    const { data: approverData, error: nameError } = await supabase.from(
      "users",
    ).select("name").eq("id", approverId).single();
    if (nameError) throw nameError;
    const { error } = await supabase.from("extra_work_logs").update({
      status: "Rejected",
      approver_id: approverId,
      approver_name: approverData.name,
      rejection_reason: reason,
    }).eq("id", claimId);
    if (error) throw error;
  },
  getManpowerDetails: async (siteId: string): Promise<ManpowerDetail[]> => {
    const { data, error } = await supabase.rpc("get_manpower_details", {
      site_id_param: siteId,
    });
    if (error) throw error;
    return toCamelCase(data);
  },
  // Fix: Add all missing functions to the api object.
  addCompOffLog: async (
    logData: Omit<CompOffLog, "id">,
  ): Promise<CompOffLog> => {
    const { data, error } = await supabase.from("comp_off_logs").insert(
      toSnakeCase(logData),
    ).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  exportAllData: async (): Promise<any> => {
    const tables = [
      "users",
      "organizations",
      "onboarding_submissions",
      "settings",
    ];
    const data: Record<string, any> = {};
    for (const table of tables) {
      const { data: tableData, error } = await supabase.from(table).select("*");
      if (error) throw new Error(`Failed to export ${table}`);
      data[table] = tableData;
    }
    return toCamelCase(data);
  },
  getPincodeDetails: async (
    pincode: string,
  ): Promise<{ city: string; state: string }> => {
    // This is a mock. A real implementation would use an external API.
    return new Promise((resolve) =>
      setTimeout(() => resolve({ city: "Bengaluru", state: "Karnataka" }), 500)
    );
  },
  suggestDepartmentForDesignation: async (
    designation: string,
  ): Promise<string | null> => {
    const mapping: Record<string, string> = {
      "Security Guard": "Security",
      "Housekeeping Staff": "Housekeeping",
      "Site Manager": "Management",
    };
    const key = Object.keys(mapping).find((key) =>
      designation.toLowerCase().includes(key.toLowerCase())
    );
    return Promise.resolve(key ? mapping[key] : null);
  },
  verifyBankAccountWithPerfios: async (
    data: PerfiosVerificationData,
  ): Promise<VerificationResult> => {
    console.log("Mock verifying bank account:", data);
    await new Promise((resolve) => setTimeout(resolve, 1500));
    const success = Math.random() > 0.1; // 90% success rate
    return {
      success,
      message: success
        ? "Bank account verified successfully."
        : "Account holder name did not match.",
      verifiedFields: {
        name: null,
        dob: null,
        aadhaar: null,
        bank: success,
        uan: null,
        esi: null,
        accountHolderName: success,
        accountNumber: success,
        ifscCode: true,
      },
    };
  },
  verifyAadhaar: async (aadhaar: string): Promise<VerificationResult> => {
    console.log("Mock verifying Aadhaar:", aadhaar);
    await new Promise((resolve) => setTimeout(resolve, 1000));
    const success = aadhaar.length === 12 && Math.random() > 0.1;
    return {
      success,
      message: success
        ? "Aadhaar details verified."
        : "Invalid Aadhaar number.",
      verifiedFields: {
        name: null,
        dob: null,
        aadhaar: success,
        bank: null,
        uan: null,
        esi: null,
      },
    };
  },
  lookupUan: async (uan: string): Promise<VerificationResult> => {
    console.log("Mock looking up UAN:", uan);
    await new Promise((resolve) => setTimeout(resolve, 1200));
    const success = uan.length === 12 && Math.random() > 0.1;
    return {
      success,
      message: success
        ? "UAN found and linked."
        : "UAN not found in EPFO database.",
      verifiedFields: {
        name: null,
        dob: null,
        aadhaar: null,
        bank: null,
        uan: success,
        esi: null,
      },
    };
  },
  extractDataFromImage: async (
    base64: string,
    mimeType: string,
    schema: any,
    docType?: string,
  ): Promise<any> => {
    // If AI is not available return an empty object.  This prevents the
    // application from crashing when running without an API key.  Otherwise,
    // call the Gemini model to extract structured data from the image.
    if (!ai) {
      console.warn(
        "AI feature disabled: cannot extract data from image. Returning empty result.",
      );
      return {};
    }
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: {
        parts: [
          {
            text:
              `Extract the structured data from this document image. It is a ${
                docType || "document"
              }.`,
          },
          { inlineData: { data: base64, mimeType } },
        ],
      },
      config: {
        responseMimeType: "application/json",
        responseSchema: schema,
      },
    });
    const jsonStr = response.text.trim();
    return JSON.parse(jsonStr);
  },
  crossVerifyNames: async (
    name1: string,
    name2: string,
  ): Promise<{ isMatch: boolean; reason: string }> => {
    // Without the AI client fall back to a simple case-insensitive
    // comparison.  Return a basic match result with a reason.  This ensures
    // the application continues to operate without a Google API key.
    if (!ai) {
      const isMatch = name1.trim().toLowerCase() === name2.trim().toLowerCase();
      return {
        isMatch,
        reason: isMatch
          ? "Names are identical (case-insensitive match) without AI."
          : "AI disabled; simple case-insensitive comparison used.",
      };
    }
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents:
        `Are these two names referring to the same person? Name 1: "${name1}", Name 2: "${name2}". Respond in JSON format with two keys: "isMatch" (boolean) and "reason" (a brief string explanation).`,
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            isMatch: { type: Type.BOOLEAN },
            reason: { type: Type.STRING },
          },
        },
      },
    });
    const jsonStr = response.text.trim();
    return JSON.parse(jsonStr);
  },
  verifyFingerprintImage: async (
    base64: string,
    mimeType: string,
  ): Promise<{ containsFingerprints: boolean; reason: string }> => {
    // When AI is unavailable return a default response indicating that no
    // fingerprints were detected.  This fallback prevents runtime errors when
    // running the project without a Gemini API key.
    if (!ai) {
      return {
        containsFingerprints: false,
        reason: "AI disabled; cannot detect fingerprints.",
      };
    }
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash",
      contents: {
        parts: [
          {
            text:
              'Does this image contain one or more human fingerprints? The image might be a scan from paper. Respond in JSON with "containsFingerprints" (boolean) and "reason" (string).',
          },
          { inlineData: { data: base64, mimeType } },
        ],
      },
      config: {
        responseMimeType: "application/json",
        responseSchema: {
          type: Type.OBJECT,
          properties: {
            containsFingerprints: { type: Type.BOOLEAN },
            reason: { type: Type.STRING },
          },
        },
      },
    });
    const jsonStr = response.text.trim();
    return JSON.parse(jsonStr);
  },
  enhanceDocumentPhoto: async (
    base64: string,
    mimeType: string,
  ): Promise<string> => {
    // When AI is unavailable simply return the original image.  This ensures
    // document uploads still work without enhancement.
    if (!ai) {
      console.warn(
        "AI disabled; returning original document photo without enhancement.",
      );
      return base64;
    }
    const response = await ai.models.generateContent({
      model: "gemini-2.5-flash-image",
      contents: {
        parts: [
          {
            text:
              "Enhance this document photo. Improve contrast, correct perspective to be flat, and make text as clear as possible. Return only the enhanced image.",
          },
          { inlineData: { data: base64, mimeType } },
        ],
      },
      config: {
        responseModalities: [Modality.IMAGE],
      },
    });
    for (const part of response.candidates[0].content.parts) {
      if (part.inlineData) {
        return part.inlineData.data;
      }
    }
    throw new Error("AI did not return an enhanced image.");
  },
  getSiteStaffDesignations: async (): Promise<SiteStaffDesignation[]> => {
    const { data, error } = await supabase.from("site_staff_designations")
      .select("*");
    if (error) throw error;
    return (data || []).map(toCamelCase);
  },
  updateManpowerDetails: async (
    siteId: string,
    details: ManpowerDetail[],
  ): Promise<void> => {
    console.log("Mock updating manpower for", siteId, details);
    await new Promise((resolve) => setTimeout(resolve, 500));
  },
  getCompOffLogs: async (userId: string): Promise<CompOffLog[]> => {
    const query = supabase.from("comp_off_logs").select("*").eq(
      "user_id",
      userId,
    ).order("date_earned", { ascending: false });
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  checkCompOffTableExists: async (): Promise<void> => {
    const { error } = await supabase.from("comp_off_logs").select("id").limit(
      1,
    );
    if (error) throw error;
  },
  getAllSiteAssets: async (): Promise<Record<string, Asset[]>> => {
    return Promise.resolve({}); // mock
  },
  updateSiteAssets: async (siteId: string, assets: Asset[]): Promise<void> => {
    console.log("Mock updating assets for site", siteId, assets);
    return Promise.resolve();
  },
  getBackOfficeIdSeries: async (): Promise<BackOfficeIdSeries[]> => {
    const { data, error } = await supabase.from("settings").select(
      "back_office_id_series",
    ).eq("id", "singleton").single();
    if (error) throw error;
    return data?.back_office_id_series
      ? toCamelCase(data.back_office_id_series)
      : [];
  },
  updateBackOfficeIdSeries: async (
    series: BackOfficeIdSeries[],
  ): Promise<void> => {
    const { error } = await supabase.from("settings").update({
      back_office_id_series: toSnakeCase(series),
    }).eq("id", "singleton");
    if (error) throw error;
  },
  updateSiteStaffDesignations: async (
    designations: SiteStaffDesignation[],
  ): Promise<void> => {
    const { error } = await supabase.from("site_staff_designations").upsert(
      toSnakeCase(designations),
      { onConflict: "id" },
    );
    if (error) throw error;
  },
  getAllSiteIssuedTools: async (): Promise<Record<string, IssuedTool[]>> => {
    return Promise.resolve({});
  },
  getToolsList: async (): Promise<MasterToolsList> => {
    return Promise.resolve({});
  },
  updateSiteIssuedTools: async (
    siteId: string,
    tools: IssuedTool[],
  ): Promise<void> => {
    console.log("Mock updating tools for site", siteId, tools);
  },
  getAllSiteGentsUniforms: async (): Promise<
    Record<string, SiteGentsUniformConfig>
  > => {
    return Promise.resolve({});
  },
  getMasterGentsUniforms: async (): Promise<MasterGentsUniforms> => {
    return Promise.resolve({ pants: [], shirts: [] });
  },
  updateSiteGentsUniforms: async (
    siteId: string,
    config: SiteGentsUniformConfig,
  ): Promise<void> => {
    console.log("Mock updating gents uniforms for", siteId, config);
  },
  getAllSiteUniformDetails: async (): Promise<
    Record<string, SiteUniformDetailsConfig>
  > => {
    return Promise.resolve({});
  },
  updateSiteUniformDetails: async (
    siteId: string,
    config: SiteUniformDetailsConfig,
  ): Promise<void> => {
    console.log("Mock updating uniform details for", siteId, config);
  },
  getAllSiteLadiesUniforms: async (): Promise<
    Record<string, SiteLadiesUniformConfig>
  > => {
    return Promise.resolve({});
  },
  getMasterLadiesUniforms: async (): Promise<MasterLadiesUniforms> => {
    return Promise.resolve({ pants: [], shirts: [] });
  },
  updateSiteLadiesUniforms: async (
    siteId: string,
    config: SiteLadiesUniformConfig,
  ): Promise<void> => {
    console.log("Mock updating ladies uniforms for", siteId, config);
  },
  getUniformRequests: async (): Promise<UniformRequest[]> => {
    const query = supabase.from("uniform_requests").select("*");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  submitUniformRequest: async (
    request: UniformRequest,
  ): Promise<UniformRequest> => {
    const { data, error } = await supabase.from("uniform_requests").insert(
      toSnakeCase(request),
    ).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  updateUniformRequest: async (
    request: UniformRequest,
  ): Promise<UniformRequest> => {
    const { data, error } = await supabase.from("uniform_requests").update(
      toSnakeCase(request),
    ).eq("id", request.id).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  deleteUniformRequest: async (id: string): Promise<void> => {
    const { error } = await supabase.from("uniform_requests").delete().eq(
      "id",
      id,
    );
    if (error) throw error;
  },
  getInvoiceStatuses: async (
    date: Date,
  ): Promise<
    Record<string, "Not Generated" | "Generated" | "Sent" | "Paid">
  > => {
    console.log("Mock fetching invoice statuses for", date);
    return Promise.resolve({});
  },
  getInvoiceSummaryData: async (
    siteId: string,
    date: Date,
  ): Promise<InvoiceData> => {
    console.log("Mock fetching invoice data for", siteId, date);
    return Promise.resolve({
      siteName: "Mock Site",
      siteAddress: "Mock Address",
      invoiceNumber: "INV-001",
      invoiceDate: "2023-01-31",
      statementMonth: "January-2023",
      lineItems: [],
    });
  },
  getSupportTickets: async (): Promise<SupportTicket[]> => {
    const query = supabase.from("support_tickets").select(
      "*, posts:ticket_posts(*, comments:ticket_comments(*))",
    );
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
  getSupportTicketById: async (id: string): Promise<SupportTicket | null> => {
    const { data, error } = await supabase.from("support_tickets").select(
      "*, posts:ticket_posts(*, comments:ticket_comments(*))",
    ).eq("id", id).single();
    if (error) throw error;
    return toCamelCase(data);
  },
  createSupportTicket: async (
    ticketData: Partial<SupportTicket>,
  ): Promise<SupportTicket> => {
    // If an attachment was provided with a File object, upload it to the support attachments bucket
    let attachmentPath = null;
    const attachment: any = (ticketData as any).attachment;
    if (attachment && attachment.file instanceof File) {
      try {
        const { path } = await api.uploadDocument(
          attachment.file as File,
          SUPPORT_ATTACHMENTS_BUCKET,
        );
        attachmentPath = path;
      } catch (uploadErr) {
        console.error("Failed to upload support ticket attachment:", uploadErr);
      }
    }

    // Remove complex attachment object from payload
    delete (ticketData as any).attachment;

    // Append attachment URL to description if exists (since no attachment_url column)
    let finalDescription = ticketData.description || "";
    if (attachmentPath) {
      // Construct public URL if possible or just path
      finalDescription += `\n\n[Attachment: ${attachmentPath}]`;
    }

    // Ensure strict timestamps and required fields
    const cleanPayload = {
      ...ticketData,
      description: finalDescription,
      raisedAt: new Date().toISOString(),
      ticketNumber: `TKT-${Date.now()}`, // Generate ticket number as it is required
    };
    // Note: updatedAt is NOT sent because the support_tickets table does not have an updated_at column.

    const { data, error } = await supabase.from("support_tickets").insert(
      toSnakeCase(cleanPayload),
    ).select("*, posts:ticket_posts(*, comments:ticket_comments(*))").single();
    if (error) throw error;
    return toCamelCase(data);
  },
  updateSupportTicket: async (
    id: string,
    updates: Partial<SupportTicket>,
  ): Promise<SupportTicket> => {
    // Handle attachment upload when updating a ticket
    const attachment: any = (updates as any).attachment;
    if (attachment && attachment.file instanceof File) {
      try {
        const { path } = await api.uploadDocument(
          attachment.file as File,
          SUPPORT_ATTACHMENTS_BUCKET,
        );
        (updates as any).attachment = {
          name: attachment.name,
          type: attachment.type,
          size: attachment.size,
          path,
        };
      } catch (uploadErr) {
        console.error(
          "Failed to upload updated support ticket attachment:",
          uploadErr,
        );
        delete (updates as any).attachment;
      }
    }
    const { data, error } = await supabase.from("support_tickets").update(
      toSnakeCase(updates),
    ).eq("id", id).select(
      "*, posts:ticket_posts(*, comments:ticket_comments(*))",
    ).single();
    if (error) throw error;
    return toCamelCase(data);
  },
  deleteSupportTicket: async (id: string): Promise<void> => {
    const { error } = await supabase.from("support_tickets").delete().eq(
      "id",
      id,
    );
    if (error) throw error;
  },
  addTicketPost: async (
    ticketId: string,
    postData: Partial<TicketPost>,
  ): Promise<TicketPost> => {
    const { data, error } = await supabase.from("ticket_posts").insert(
      toSnakeCase(postData),
    ).select("*, comments:ticket_comments(*)").single();
    if (error) throw error;
    return toCamelCase(data);
  },
  togglePostLike: async (postId: string, userId: string): Promise<void> => {
    const { data, error } = await supabase.from("ticket_posts").select("likes")
      .eq("id", postId).single();
    if (error) throw error;
    const likes = (data.likes as string[]) || [];
    const newLikes = likes.includes(userId)
      ? likes.filter((id) => id !== userId)
      : [...likes, userId];
    const { error: updateError } = await supabase.from("ticket_posts").update({
      likes: newLikes,
    }).eq("id", postId);
    if (updateError) throw updateError;
  },
  addPostComment: async (
    postId: string,
    commentData: Partial<TicketComment>,
  ): Promise<TicketComment> => {
    const { data, error } = await supabase.from("ticket_comments").insert(
      toSnakeCase(commentData),
    ).select().single();
    if (error) throw error;
    return toCamelCase(data);
  },
  getVerificationCostBreakdown: async (
    startDate: string,
    endDate: string,
  ): Promise<SubmissionCostBreakdown[]> => {
    const query = supabase.from("onboarding_submissions")
      .select("id, employee_id, personal, enrollment_date, verification_usage")
      .gte("enrollment_date", startDate).lte("enrollment_date", endDate);
    const data = await fetchAllPaginated(query);

    return (data || []).map((sub) => {
      const camelSub = toCamelCase(sub);
      return {
        id: camelSub.id,
        employeeId: camelSub.personal.employeeId,
        employeeName:
          `${camelSub.personal.firstName} ${camelSub.personal.lastName}`,
        enrollmentDate: camelSub.enrollmentDate,
        totalCost: 0, // Will be calculated on the frontend
        breakdown: camelSub.verificationUsage || [],
      };
    });
  },

  generatePdf: async (
    content: string | HTMLElement,
    options: any,
  ): Promise<void> => {
    try {
      // @ts-ignore
      const html2pdf = (await import("html2pdf.js")).default;
      await html2pdf().set(options).from(content).save();
    } catch (error) {
      console.error("PDF generation failed:", error);
      throw error;
    }
  },

  /**
   * Batch resolve addresses for a list of coordinates.
   * Checks Supabase 'location_cache' first, then fetches from Nominatim for missing ones,
   * and caches the results.
   */
  batchResolveAddresses: async (
    coords: { lat: number; lon: number }[],
  ): Promise<Record<string, string>> => {
    if (coords.length === 0) return {};

    // Normalize coordinates to 6 decimal places for consistent key generation
    const normalizeCoord = (n: number) => parseFloat(n.toFixed(6));

    const uniqueCoords = Array.from(
      new Set(
        coords.map((c) => `${normalizeCoord(c.lat)},${normalizeCoord(c.lon)}`),
      ),
    )
      .map((s) => {
        const [lat, lon] = s.split(",").map(Number);
        return { lat, lon };
      });

    const resultMap: Record<string, string> = {};
    const missingCoords: { lat: number; lon: number }[] = [];

    // 1. Check Cache in Supabase
    // Fetch cache entries that match any of the latitudes (approximate filter)
    const lats = uniqueCoords.map((c) => c.lat);
    const { data: cachedData, error } = await supabase
      .from("location_cache")
      .select("*")
      .in("latitude", lats);

    if (!error && cachedData) {
      cachedData.forEach((row: any) => {
        // Normalize database values for comparison
        const dbLat = normalizeCoord(parseFloat(row.latitude));
        const dbLon = normalizeCoord(parseFloat(row.longitude));
        const key = `${dbLat},${dbLon}`;

        // Verify exact match including longitude
        if (
          uniqueCoords.some((c) =>
            normalizeCoord(c.lat) === dbLat && normalizeCoord(c.lon) === dbLon
          )
        ) {
          resultMap[key] = row.address;
        }
      });
    }

    // 2. Identify Missing
    uniqueCoords.forEach((c) => {
      const key = `${normalizeCoord(c.lat)},${normalizeCoord(c.lon)}`;
      if (!resultMap[key]) {
        missingCoords.push(c);
      }
    });

    console.log(
      "Geocoding API: Found",
      Object.keys(resultMap).length,
      "cached,",
      missingCoords.length,
      "missing",
    );

    // 3. Fetch Missing from Nominatim & Cache
    // We must rate limit this to avoid banning. 1 request per second is safe.
    for (const coord of missingCoords) {
      try {
        const address = await import("../utils/locationUtils").then((m) =>
          m.reverseGeocode(coord.lat, coord.lon)
        );

        const key = `${normalizeCoord(coord.lat)},${normalizeCoord(coord.lon)}`;
        resultMap[key] = address;

        // Insert into Cache
        await supabase.from("location_cache").insert({
          latitude: normalizeCoord(coord.lat),
          longitude: normalizeCoord(coord.lon),
          address: address,
        });

        console.log("Geocoding API: Fetched and cached", key, "->", address);

        // Delay to respect API rate limits
        await new Promise((r) => setTimeout(r, 1000));
      } catch (e) {
        console.error(
          `Failed to resolve address for ${coord.lat},${coord.lon}`,
          e,
        );
        const key = `${normalizeCoord(coord.lat)},${normalizeCoord(coord.lon)}`;
        resultMap[key] = `${coord.lat.toFixed(4)}, ${coord.lon.toFixed(4)}`;
      }
    }

    return resultMap;
  },

  /**
   * Send a security alert notification to the user's reporting manager about
   * developer mode, location spoofing, or other security violations.
   */
  sendSecurityAlert: async (
    userId: string,
    userName: string,
    violationType: "developer_mode" | "location_spoofing",
    deviceInfo?: string,
  ): Promise<void> => {
    // Get the user's reporting manager
    const { data: userData, error: userError } = await supabase
      .from("users")
      .select("reporting_manager_id")
      .eq("id", userId)
      .single();

    if (userError || !userData?.reporting_manager_id) {
      console.warn("Could not find reporting manager for security alert");
      return;
    }

    const violationMessage = violationType === "developer_mode"
      ? "attempted to access the application with Developer Mode enabled"
      : "attempted to access the application with Location Spoofing detected";

    const deviceText = deviceInfo ? ` using device: ${deviceInfo}` : "";

    await api.createNotification({
      userId: userData.reporting_manager_id,
      title: "🔒 Security Alert",
      message:
        `${userName} ${violationMessage}${deviceText}. Access was blocked for security reasons.`,
      type: "security",
      link: `/user-management`, // Link to user management or security dashboard
    });
  },

  /**
   * Send a device change alert notification to the user's reporting manager.
   */
  sendDeviceChangeAlert: async (
    userId: string,
    userName: string,
    oldDevice: string,
    newDevice: string,
  ): Promise<void> => {
    // Get the user's reporting manager
    const { data: userData, error: userError } = await supabase
      .from("users")
      .select("reporting_manager_id")
      .eq("id", userId)
      .single();

    if (userError || !userData?.reporting_manager_id) {
      console.warn("Could not find reporting manager for device change alert");
      return;
    }

    await api.createNotification({
      userId: userData.reporting_manager_id,
      title: "📱 Device Change Detected",
      message:
        `${userName} logged in from a new device. Previous device: ${oldDevice}, New device: ${newDevice}`,
      type: "info",
      link: `/user-management`,
    });
  },

  // --- Security Patrolling ---
  getPatrolQrCodes: async (siteId?: string): Promise<PatrolQRCode[]> => {
    let query = supabase.from("patrol_qr_codes").select("*");
    if (siteId) query = query.eq("site_id", siteId);
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },

  createPatrolQrCode: async (
    data: Partial<PatrolQRCode>,
  ): Promise<PatrolQRCode> => {
    const { data: created, error } = await supabase.from("patrol_qr_codes")
      .insert(
        toSnakeCase(data),
      ).select().single();
    if (error) throw error;
    return toCamelCase(created);
  },

  updatePatrolQrCode: async (
    id: string,
    updates: Partial<PatrolQRCode>,
  ): Promise<void> => {
    const { error } = await supabase.from("patrol_qr_codes").update(
      toSnakeCase(updates),
    ).eq("id", id);
    if (error) throw error;
  },

  deletePatrolQrCode: async (id: string): Promise<void> => {
    const { error } = await supabase.from("patrol_qr_codes").delete().eq(
      "id",
      id,
    );
    if (error) throw error;
  },

  // --- My Team / Location Tracking ---

  /**
   * Log user's current location (background or foreground).
   */
  logLocation: async (data: Partial<LocationLog>): Promise<void> => {
    const { error } = await supabase.from("user_location_logs").insert(
      toSnakeCase(data),
    );
    if (error) throw error;
  },

  /**
   * Fetch team members (users reporting to current user).
   * For admins, this could be adjusted to return all users or filter by team.
   * Currently, it fetches users where reporting_manager_id matches the param.
   */
  getTeamMembers: async (managerId: string): Promise<User[]> => {
    const query = supabase
      .from("users")
      .select("*")
      .eq("reporting_manager_id", managerId);

    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },

  /**
   * Fetch location history for a user on a specific date.
   */
  getLocationHistory: async (
    userId: string,
    date: string,
  ): Promise<LocationLog[]> => {
    const start = new Date(date);
    start.setHours(0, 0, 0, 0);
    const end = new Date(date);
    end.setHours(23, 59, 59, 999);

    const query = supabase
      .from("user_location_logs")
      .select("*")
      .eq("user_id", userId)
      .gte("timestamp", start.toISOString())
      .lte("timestamp", end.toISOString())
      .order("timestamp", { ascending: true }); // Ordered for route plotting

    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },

  /**
   * Fetch the latest known location for a list of users (within last 24h).
   */
  getLatestLocations: async (
    userIds: string[],
  ): Promise<Record<string, LocationLog>> => {
    if (userIds.length === 0) return {};

    // Look back 24 hours
    const oneDayAgo = new Date();
    oneDayAgo.setHours(oneDayAgo.getHours() - 24);

    const query = supabase
      .from("user_location_logs")
      .select("*")
      .in("user_id", userIds)
      .gte("timestamp", oneDayAgo.toISOString())
      .order("timestamp", { ascending: false });

    const data = await fetchAllPaginated(query);

    const latestMap: Record<string, LocationLog> = {};
    (data || []).forEach((log) => {
      const camelLog = toCamelCase(log) as LocationLog;
      if (!latestMap[camelLog.userId]) {
        latestMap[camelLog.userId] = camelLog;
      }
    });
    return latestMap;
  },

  submitPatrolLog: async (log: Omit<PatrolLog, "id">): Promise<void> => {
    // If photo is provided as data URL, upload it first
    let photoUrl = log.photoUrl;
    if (photoUrl && photoUrl.startsWith("data:")) {
      const { data: { session } } = await supabase.auth.getSession();
      if (!session?.user) throw new Error("User not authenticated");

      const blob = await dataUrlToBlob(photoUrl);
      const fileExt = "jpg";
      const filePath = `${session.user.id}/patrols/${Date.now()}.${fileExt}`;
      const { error: uploadError } = await supabase.storage.from(
        "task-attachments",
      ).upload(filePath, blob);

      if (uploadError) throw uploadError;
      const { data: { publicUrl } } = supabase.storage.from("task-attachments")
        .getPublicUrl(filePath);
      photoUrl = publicUrl;
    }

    const dbLog = {
      ...toSnakeCase(log),
      photo_url: photoUrl,
    };

    const { error } = await supabase.from("patrol_logs").insert(dbLog);
    if (error) throw error;
  },

  getPatrolLogs: async (
    date: string,
    siteId?: string,
  ): Promise<PatrolLog[]> => {
    // filtering by siteId would require a join, use explicit join syntax or multiple queries
    // For simplicity, we assume we might need a custom RPC or just fetch all for now if site filtering is complex via JS SDK without foreign key definitions in generated types
    // Convert local date string (YYYY-MM-DD) to start/end of day in UTC
    // Appending T00:00:00 ensures it is treated as local time by Date constructor
    const startOfDay = new Date(`${date}T00:00:00`);
    const endOfDay = new Date(`${date}T23:59:59.999`);

    let query = supabase.from("patrol_logs").select(
      "*, patrol_qr_codes!inner(site_id)",
    )
      .gte("scan_time", startOfDay.toISOString())
      .lte("scan_time", endOfDay.toISOString())
      .order("scan_time", { ascending: false });

    if (siteId) {
      query = query.eq("patrol_qr_codes.site_id", siteId);
    }

    const data = await fetchAllPaginated(query);
    // map the inner joined data if needed, or just return flattened
    // The toCamelCase helper might need adjustment if nested objects return
    return (data || []).map(toCamelCase);
  },

  getPatrolDailyScores: async (date: string): Promise<PatrolDailyScore[]> => {
    const query = supabase.from("patrol_daily_scores")
      .select("*")
      .eq("date", date);
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },

  getCheckpoints: async (siteId: string): Promise<PatrolQRCode[]> => {
    const query = supabase.from("patrol_qr_codes").select("*")
      .eq("site_id", siteId).eq("status", "active");
    const data = await fetchAllPaginated(query);
    return (data || []).map(toCamelCase);
  },
};
